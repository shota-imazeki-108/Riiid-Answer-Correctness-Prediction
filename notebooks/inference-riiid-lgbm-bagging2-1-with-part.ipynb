{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = False","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport os","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from collections import defaultdict\n# import datatable as dt\nimport lightgbm as lgb\n# from matplotlib import pyplot as plt\nimport riiideducation\n# from sklearn.metrics import roc_auc_score\nimport gc\n\n_ = np.seterr(divide='ignore', invalid='ignore')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clfs = []\n\nimport joblib\nclfs.append(joblib.load('../input/riiid-inference-part/lgb3.pkl'))\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'answered_correctly'\npickles_path = '../input/riiid-pickle'","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lagtime_in_lecture_mean = 20061832.085456338\nlag_elapsed_time_mean = 20416566.858529676\nlagtime_mean = 20441941.566456214\nlagtime_mean2 = 40370652.30368494\nlagtime_mean3 = 60008399.99610778\nprior_question_elapsed_time_mean = 13005.0810546875","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# origin\nfeatures_dict = {\n    #'user_id',\n    'timestamp':'float16',#\n    'user_interaction_count':'int16',\n    'user_interaction_timestamp_mean':'float32',\n    'lagtime':'float32',#\n    'lagtime2':'float32',\n    'lagtime3':'float32',\n    #'lagtime_mean':'int32',\n    'content_id':'int16',\n    'task_container_id':'int16',\n    'user_lecture_sum':'int16',#\n    'user_lecture_lv':'float16',##\n    'prior_question_elapsed_time':'float32',#\n    'delta_prior_question_elapsed_time':'int32',#\n    'user_correctness':'float16',#\n    'user_uncorrect_count':'int16',#\n    'user_correct_count':'int16',#\n    #'content_correctness':'float16',\n    'content_correctness_std':'float16',\n    'content_correct_count':'int32',\n    'content_uncorrect_count':'int32',#\n    'content_elapsed_time_mean':'float16',\n    'content_had_explanation_mean':'float16',\n    'content_explation_false_mean':'float16',\n    'content_explation_true_mean':'float16',\n    'task_container_correctness':'float16',\n    'task_container_std':'float16',\n    'task_container_cor_count':'int32',#\n    'task_container_uncor_count':'int32',#\n    'attempt_no':'int8',#\n    'part':'int8',\n    'part_correctness_mean':'float16',\n    'part_correctness_std':'float16',\n    'part_uncor_count':'int32',\n    'part_cor_count':'int32',\n    'tags0': 'int8',\n    'tags1': 'int8',\n    'tags2': 'int8',\n    'tags3': 'int8',\n    'tags4': 'int8',\n    'tags5': 'int8',\n   # 'tags6': 'int8',\n   # 'tags7': 'int8',\n#     'tags0_correctness_mean':'float16',\n#     'tags1_correctness_mean':'float16',\n#     'tags2_correctness_mean':'float16',\n#     'tags4_correctness_mean':'float16',\n#     'bundle_id':'int16',\n#     'bundle_correctness_mean':'float16',\n#     'bundle_uncor_count':'int32',\n#     'bundle_cor_count':'int32',\n    'part_bundle_id':'int32',\n    'content_sub_bundle':'int8',\n    'prior_question_had_explanation':'int8',\n    'explanation_mean':'float16', #\n    #'explanation_var',#\n    'explanation_false_count':'int16',#\n    'explanation_true_count':'int16',#\n   # 'community':'int8',\n#     'part_1',\n#     'part_2',\n#     'part_3',\n#     'part_4',\n#     'part_5',\n#     'part_6',\n#     'part_7',\n#     'type_of_concept',\n#     'type_of_intention',\n#     'type_of_solving_question',\n#     'type_of_starter'\n}\ncategorical_columns= [\n    #'user_id',\n    'content_id',\n    'task_container_id',\n    'part',\n   # 'community',\n    'tags0',\n    'tags1',\n    'tags2',\n    'tags3',\n    'tags4',\n    'tags5',\n    #'tags6',\n    #'tags7',\n    #'bundle_id',\n    'part_bundle_id',\n    'content_sub_bundle',\n    'prior_question_had_explanation', \n#     'part_1',\n#     'part_2',\n#     'part_3',\n#     'part_4',\n#     'part_5',\n#     'part_6',\n#     'part_7',\n#     'type_of_concept',\n#     'type_of_intention',\n#     'type_of_solving_question',\n#     'type_of_starter'\n]\n\nfeatures=list(features_dict.keys())\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### Value>100000\nfeatures_dict = {\n#  'lagtime': 'float32', # curr\n 'lagtime2': 'float32', #\n 'lagtime3': 'float32',\n 'content_id': 'int16',\n#  'task_container_id': 'int16', # curr\n 'prior_question_elapsed_time': 'float32',\n 'user_correctness': 'float16',\n#  'user_uncorrect_count': 'int16', # curr\n 'user_correct_count': 'int16',\n 'content_correctness_std': 'float16',\n 'content_correct_count': 'int32',\n 'content_uncorrect_count': 'int32',\n 'content_had_explanation_mean': 'float16', #\n#  'content_explation_false_mean': 'float16', # curr\n 'content_explation_true_mean': 'float16',\n 'attempt_no': 'int8',\n#  'part': 'int8', # curr\n 'part_correctness_mean': 'float16',\n 'part_bundle_id': 'int32',\n 'explanation_false_count': 'int16',\n 'lagtime_in_lecture': 'float32',\n 'content_lagtime': 'float32',\n#  'lag_elapsed_time': 'float32', # curr\n 'attempt_no_mean': 'float16',\n 'lagtime1_2': 'float32',\n 'lagtime2_3': 'float32',\n 'user_part_correctness': 'float16',\n#  'user_part_rate': 'float16',\n 'part_lagtime': 'float16',\n 'user_lecture_sum': 'float16',\n 'user_lecture_lv': 'int16',\n#  'community': 'int16',\n#  'community_correctness_mean': 'float16',\n#  'lecture_of_day': 'int16',\n#  'user_interaction_lagtime_mean': 'float32',\n#  'lagtime_cumsum': 'int16'\n}\n\nfeatures=list(features_dict.keys())\n\ndelete_col = set(categorical_columns) - set(features)\ncategorical_columns = list(set(categorical_columns) - delete_col)\nprint(categorical_columns)","execution_count":8,"outputs":[{"output_type":"stream","text":"['content_id', 'part_bundle_id']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('start Inference')","execution_count":9,"outputs":[{"output_type":"stream","text":"start Inference\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lagtime_sum_dict = lagtime_agg['sum'].astype('int16').to_dict(defaultdict(int))\n# lagtime_count_dict = lagtime_agg['count'].astype('int16').to_dict(defaultdict(int))\n\n# del lagtime_agg","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions_df = pd.read_pickle(os.path.join(pickles_path, 'questions_df.pkl'))\n# lecture情報取り込む際に邪魔になる\nquestions_df.drop('part', axis=1)\n\npart_lecture_user_agg = pd.read_pickle('../input/riiid-inference-part/part_lecture_user_agg.pkl')\npart_lecture_user_sum_dict = part_lecture_user_agg['sum'].astype('int16').to_dict(defaultdict(int))\npart_lecture_user_count_dict = part_lecture_user_agg['count'].astype('int16').to_dict(defaultdict(int))\n\ndel part_lecture_user_agg","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_timestamp_u_part = pd.read_pickle('../input/riiid-inference-part/max_timestamp_u_part_org.pkl')\nmax_timestamp_u_part_dict = max_timestamp_u_part['timestamp'].to_dict(defaultdict(int))\ndel max_timestamp_u_part\n\npart_user_agg = pd.read_pickle('../input/riiid-inference-part/part_user_agg.pkl')\npart_user_sum_dict = part_user_agg['sum'].astype('int16').to_dict(defaultdict(int))\npart_user_count_dict = part_user_agg['count'].astype('int16').to_dict(defaultdict(int))\n\ndel part_user_agg","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_timestamp_u_content = pd.read_pickle(os.path.join(pickles_path, 'max_timestamp_u_content.pkl'))\nmax_timestamp_u_content_dict = max_timestamp_u_content['timestamp'].to_dict(defaultdict(int))\ndel max_timestamp_u_content","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_agg = pd.read_pickle(os.path.join(pickles_path, 'user_agg.pkl'))\nuser_sum_dict = user_agg['sum'].astype('int16').to_dict(defaultdict(int))\nuser_count_dict = user_agg['count'].astype('int16').to_dict(defaultdict(int))\n# user_sum_dict2 = user_agg2['sum'].astype('int16').to_dict(defaultdict(int))\n# user_count_dict2 = user_agg2['count'].astype('int16').to_dict(defaultdict(int))\n\ndel user_agg","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#user_var_dict = user_agg['var'].astype('float16').to_dict(defaultdict(int))\n\n# content_sum_dict = content_agg['sum'].astype('int32').to_dict(defaultdict(int))\n# content_count_dict = content_agg['count'].astype('int32').to_dict(defaultdict(int))\n\n#del user_agg2\n#del content_agg\ngc.collect()\n\ntask_container_agg = pd.read_pickle(os.path.join(pickles_path, 'task_container_agg.pkl'))\ntask_container_sum_dict = task_container_agg['sum'].astype('int32').to_dict(defaultdict(int))\ntask_container_count_dict = task_container_agg['count'].astype('int32').to_dict(defaultdict(int))\ntask_container_std_dict = task_container_agg['var'].astype('float16').to_dict(defaultdict(int))\n\nexplanation_agg = pd.read_pickle(os.path.join(pickles_path, 'explanation_agg.pkl'))\nexplanation_sum_dict = explanation_agg['sum'].astype('int16').to_dict(defaultdict(int))\nexplanation_count_dict = explanation_agg['count'].astype('int16').to_dict(defaultdict(int))\n#explanation_var_dict = explanation_agg['var'].astype('float16').to_dict(defaultdict(int))\ndel task_container_agg\ndel explanation_agg\ngc.collect()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#task_container_std_dict","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# user_lecture_sum_dict = user_lecture_agg['sum'].astype('int16').to_dict(defaultdict(int))\n# user_lecture_count_dict = user_lecture_agg['count'].astype('int16').to_dict(defaultdict(int))\n\n#lagtime_mean_dict = lagtime_agg['mean'].astype('int32').to_dict(defaultdict(int))\n#del prior_question_elapsed_time_agg\n# del user_lecture_agg\n#del lagtime_agg\ngc.collect()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"40"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_timestamp_u = pd.read_pickle(os.path.join(pickles_path, 'max_timestamp_u.pkl'))\nmax_timestamp_u2 = pd.read_pickle(os.path.join(pickles_path, 'max_timestamp_u2.pkl'))\nmax_timestamp_u3 = pd.read_pickle(os.path.join(pickles_path, 'max_timestamp_u3.pkl'))\nuser_prior_question_elapsed_time = pd.read_pickle(os.path.join(pickles_path, 'user_prior_question_elapsed_time.pkl'))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_timestamp_u_dict=max_timestamp_u.set_index('user_id').to_dict()\nmax_timestamp_u_dict2=max_timestamp_u2.set_index('user_id').to_dict()\nmax_timestamp_u_dict3=max_timestamp_u3.set_index('user_id').to_dict()\nuser_prior_question_elapsed_time_dict=user_prior_question_elapsed_time.set_index('user_id').to_dict()\n#del question_elapsed_time_agg\ndel max_timestamp_u\ndel max_timestamp_u2\ndel max_timestamp_u3\ndel user_prior_question_elapsed_time\ngc.collect()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_timestamp_u_dict_c = max_timestamp_u_dict.copy() # lagtime_in_lecture","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attempt_no_agg = pd.read_pickle(os.path.join(pickles_path, 'attempt_no_agg.pkl'))\nattempt_no_sum_dict = attempt_no_agg['sum'].to_dict(defaultdict(int))\n\nattempt_cum_agg = pd.read_pickle(os.path.join(pickles_path, 'attempt_cum_agg.pkl'))\nattempt_cum_sum_dict = attempt_cum_agg['sum'].astype('int16').to_dict(defaultdict(int))\nattempt_cum_count_dict = attempt_cum_agg['count'].astype('int16').to_dict(defaultdict(int))\n\ndel attempt_no_agg\ngc.collect()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_max_attempt(user_id,content_id):\n    k = (user_id,content_id)\n\n    if k in attempt_no_sum_dict.keys():\n        attempt_no_sum_dict[k]+=1\n        return attempt_no_sum_dict[k]\n\n    attempt_no_sum_dict[k] = 1\n    return attempt_no_sum_dict[k]","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_content_lagtime(user_id, content_id, timestamp):\n    k = (user_id,content_id)\n\n    if k in max_timestamp_u_content_dict.keys():\n        tmp = max_timestamp_u_content_dict[k]\n        max_timestamp_u_content_dict[k]+=timestamp\n        return timestamp-tmp\n\n    max_timestamp_u_content_dict[k] = timestamp\n    return -1","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SAKT Part II"},{"metadata":{"trusted":true},"cell_type":"code","source":"#HDKIM SAKT\nimport joblib\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# MAX_SEQ = 100","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ = 250 # 250\nACCEPTED_USER_CONTENT_SIZE = 4\nEMBED_SIZE = 128\nBATCH_SIZE = 64\nDROPOUT = 0.1","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skills = joblib.load(\"/kaggle/input/riiid-sakt-model-dataset-public/skills.pkl.zip\")\nn_skill = len(skills)\ngroup = joblib.load(\"/kaggle/input/riiid-sakt-model-dataset-public/group.pkl.zip\")","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FFN(nn.Module):\n    def __init__(self, state_size = 200, forward_expansion = 1, bn_size=MAX_SEQ - 1, dropout=0.2):\n        super(FFN, self).__init__()\n        self.state_size = state_size\n        \n        self.lr1 = nn.Linear(state_size, forward_expansion * state_size)\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm1d(bn_size)\n        self.lr2 = nn.Linear(forward_expansion * state_size, state_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = self.relu(self.lr1(x))\n        x = self.bn(x)\n        x = self.lr2(x)\n        return self.dropout(x)\n\n\ndef future_mask(seq_length):\n    future_mask = (np.triu(np.ones([seq_length, seq_length]), k = 1)).astype('bool')\n    return torch.from_numpy(future_mask)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, heads = 8, dropout = DROPOUT, forward_expansion = 1):\n        super(TransformerBlock, self).__init__()\n        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=heads, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_normal = nn.LayerNorm(embed_dim)\n        self.ffn = FFN(embed_dim, forward_expansion = forward_expansion, dropout=dropout)\n        self.layer_normal_2 = nn.LayerNorm(embed_dim)\n        \n\n    def forward(self, value, key, query, att_mask):\n        att_output, att_weight = self.multi_att(value, key, query, attn_mask=att_mask)\n        att_output = self.dropout(self.layer_normal(att_output + value))\n        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n        x = self.ffn(att_output)\n        x = self.dropout(self.layer_normal_2(x + att_output))\n        return x.squeeze(-1), att_weight\n    \nclass Encoder(nn.Module):\n    def __init__(self, n_skill, max_seq=100, embed_dim=128, dropout = DROPOUT, forward_expansion = 1, num_layers=1, heads = 8):\n        super(Encoder, self).__init__()\n        self.n_skill, self.embed_dim = n_skill, embed_dim\n        self.embedding = nn.Embedding(2 * n_skill + 1, embed_dim)\n        self.pos_embedding = nn.Embedding(max_seq - 1, embed_dim)\n        self.e_embedding = nn.Embedding(n_skill+1, embed_dim)\n        self.layers = nn.ModuleList([TransformerBlock(embed_dim, forward_expansion = forward_expansion) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, question_ids):\n        device = x.device\n        x = self.embedding(x)\n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n        pos_x = self.pos_embedding(pos_id)\n        x = self.dropout(x + pos_x)\n        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n        e = self.e_embedding(question_ids)\n        e = e.permute(1, 0, 2)\n        for layer in self.layers:\n            att_mask = future_mask(e.size(0)).to(device)\n            x, att_weight = layer(e, x, x, att_mask=att_mask)\n            x = x.permute(1, 0, 2)\n        x = x.permute(1, 0, 2)\n        return x, att_weight\n\n\nclass SAKTModel(nn.Module):\n    def __init__(self, n_skill, max_seq=100, embed_dim=128, dropout = DROPOUT, forward_expansion = 1, enc_layers=1, heads = 8):\n        super(SAKTModel, self).__init__()\n        self.encoder = Encoder(n_skill, max_seq, embed_dim, dropout, forward_expansion, num_layers=enc_layers)\n        self.pred = nn.Linear(embed_dim, 1)\n        \n    def forward(self, x, question_ids):\n        x, att_weight = self.encoder(x, question_ids)\n        x = self.pred(x)\n        return x.squeeze(-1), att_weight","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    return SAKTModel(n_skill, max_seq=MAX_SEQ, embed_dim=EMBED_SIZE, forward_expansion=1, enc_layers=1, heads=8, dropout=0.1)\n\nis_gpu = torch.cuda.is_available()\nif is_gpu:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nif MAX_SEQ!=180:\n    MODEL_PATH = '../input/sakt-max-seq/sakt.pth'\nelse:\n    MODEL_PATH = '/kaggle/input/sakt-model/sakt.pth'\n\nsakt_model = create_model()\nif is_gpu:\n    sakt_model.load_state_dict(torch.load(MODEL_PATH))\nelse:\n    sakt_model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))\nsakt_model.to(device)","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"SAKTModel(\n  (encoder): Encoder(\n    (embedding): Embedding(27047, 128)\n    (pos_embedding): Embedding(249, 128)\n    (e_embedding): Embedding(13524, 128)\n    (layers): ModuleList(\n      (0): TransformerBlock(\n        (multi_att): MultiheadAttention(\n          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (layer_normal): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (ffn): FFN(\n          (lr1): Linear(in_features=128, out_features=128, bias=True)\n          (relu): ReLU()\n          (bn): BatchNorm1d(249, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (lr2): Linear(in_features=128, out_features=128, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (layer_normal_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pred): Linear(in_features=128, out_features=1, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, samples, test_df, n_skill, max_seq=100):\n        super(TestDataset, self).__init__()\n        self.samples, self.user_ids, self.test_df = samples, [x for x in test_df[\"user_id\"].unique()], test_df\n        self.n_skill, self.max_seq = n_skill, max_seq\n\n    def __len__(self):\n        return self.test_df.shape[0]\n    \n    def __getitem__(self, index):\n        test_info = self.test_df.iloc[index]\n        \n        user_id = test_info['user_id']\n        target_id = test_info['content_id']\n        \n        content_id_seq = np.zeros(self.max_seq, dtype=int)\n        answered_correctly_seq = np.zeros(self.max_seq, dtype=int)\n        \n        if user_id in self.samples.index:\n            content_id, answered_correctly = self.samples[user_id]\n            \n            seq_len = len(content_id)\n            \n            if seq_len >= self.max_seq:\n                content_id_seq = content_id[-self.max_seq:]\n                answered_correctly_seq = answered_correctly[-self.max_seq:]\n            else:\n                content_id_seq[-seq_len:] = content_id\n                answered_correctly_seq[-seq_len:] = answered_correctly\n                \n        x = content_id_seq[1:].copy()\n        x += (answered_correctly_seq[1:] == 1) * self.n_skill\n        \n        questions = np.append(content_id_seq[2:], [target_id])\n        \n        return x, questions","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = lgb.Booster(model_file='../input/riiid-lgbm-starter/model.txt')\nenv = riiideducation.make_env()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env.iter_test()\nprior_test_df = None","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" N=[0.4,0.6]","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qdf = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv')\nqdf = qdf[['question_id', 'part']]\nqdf.columns = ['content_id', 'part']\nqdf['content_type_id'] = 0\n\nldf = pd.read_csv('../input/riiid-test-answer-prediction/lectures.csv')\nldf = ldf[['lecture_id', 'part']]\nldf.columns = ['content_id', 'part']\nldf['content_type_id'] = 1\n\n# key_diff = set(lectures_df.content_id).difference(questions_df.content_id)\n# where_diff = lectures_df.content_id.isin(key_diff)\n\ncontent_df = pd.concat([qdf, ldf])\ncontent_df['join_key'] = content_df['content_type_id'].astype('str') + '_' + content_df['content_id'].astype('str')\ncontent_df = content_df[['join_key', 'part']]","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfor (test_df, sample_prediction_df) in iter_test:    \n    if prior_test_df is not None:\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop=True)\n        #prior_test_df = prior_test_df[prior_test_df[target] != -1]\n        prior_test_df['prior_question_had_explanation'].fillna(False, inplace=True)       \n        prior_test_df.prior_question_had_explanation=prior_test_df.prior_question_had_explanation.astype('int8')\n        \n        #HDKIM SAKT State Update\n        prev_group = prior_test_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n            r['content_id'].values,\n            r['answered_correctly'].values))\n        for prev_user_id in prev_group.index:\n            prev_group_content = prev_group[prev_user_id][0]\n            prev_group_ac = prev_group[prev_user_id][1]\n            if prev_user_id in group.index:\n                group[prev_user_id] = (np.append(group[prev_user_id][0],prev_group_content), \n                                       np.append(group[prev_user_id][1],prev_group_ac))\n            else:\n                group[prev_user_id] = (prev_group_content,prev_group_ac)\n            if len(group[prev_user_id][0])>MAX_SEQ:\n                new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n                new_group_ac = group[prev_user_id][1][-MAX_SEQ:]\n                group[prev_user_id] = (new_group_content,new_group_ac)\n\n        #HDKIMHDKIM\n    \n        user_ids = prior_test_df['user_id'].values\n        timestamps = prior_test_df['timestamp'].values\n        #content_ids = prior_test_df['content_id'].values\n        #task_container_ids = prior_test_df['task_container_id'].values\n        #prior_question_had_explanations = prior_test_df['prior_question_had_explanation'].values\n        targets = prior_test_df[target].values\n        attempt_nos = prior_test_df[\"attempt_no\"].values\n        parts = prior_test_df[\"part\"].values\n        \n        \n        for user_id, answered_correctly, timestamp, attempt_no, part in zip(user_ids,targets, timestamps, attempt_nos, parts):\n#             lagtime_sum_dict[user_id] = timestamp\n#             lagtime_count_dict[user_id] += 1\n            user_sum_dict[user_id] += answered_correctly\n            user_count_dict[user_id] += 1\n            attempt_cum_sum_dict[user_id] += attempt_no\n            attempt_cum_count_dict[user_id] += 1\n            \n            # part\n#             k = (user_id,part)\n            if k in part_user_sum_dict.keys():\n                part_user_sum_dict[k] += answered_correctly\n                part_user_count_dict[k] += 1\n            else:\n                part_user_sum_dict[k] = answered_correctly\n                part_user_count_dict[k] = 1\n                \n#             user_sum_dict2[user_id] += answered_correctly\n#             user_count_dict2[user_id] += 1\n        del user_ids, targets\n            \n    test_df[\"attempt_no\"] = test_df[[\"user_id\", \"content_id\"]].apply(lambda row: get_max_attempt(row[\"user_id\"], row[\"content_id\"]), axis=1)\n#     test_df=test_df.merge(questions_df.loc[questions_df.index.isin(test_df['content_id'])],\n#                   how='left', on='content_id', right_index=True)\n    test_df['join_key'] = test_df['content_type_id'].astype('str') + '_' + test_df['content_id'].astype('str')\n    test_df=test_df.merge(content_df.loc[content_df.index.isin(test_df['content_id'])],\n                  how='left', on='join_key', right_index=True)\n    prior_test_df = test_df.copy() \n    question_len=len(test_df[test_df['content_type_id'] == 0])\n    \n    \n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df.prior_question_had_explanation=test_df.prior_question_had_explanation.astype('int8')\n    test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace=True)\n    \n#     test_df[\"attempt_no\"] = test_df[[\"user_id\", \"content_id\"]].apply(lambda row: get_max_attempt(row[\"user_id\"], row[\"content_id\"]), axis=1)\n    \n\n#     user_lecture_sum = np.zeros(question_len, dtype=np.int16)\n#     user_lecture_count = np.zeros(question_len, dtype=np.int16) \n    \n#     lagtime_sum = np.zeros(question_len, dtype=np.int16)\n#     lagtime_count = np.zeros(question_len, dtype=np.int16)\n    \n    user_sum = np.zeros(question_len, dtype=np.int16)\n    user_count = np.zeros(question_len, dtype=np.int16)\n    part_user_sum = np.zeros(question_len, dtype=np.int16)\n    part_user_count = np.zeros(question_len, dtype=np.int16)\n    part_lecture_user_sum = np.zeros(question_len, dtype=np.int16)\n    part_lecture_user_count = np.zeros(question_len, dtype=np.int16)\n    attempt_sum = np.zeros(question_len, dtype=np.int16)\n    attempt_count = np.zeros(question_len, dtype=np.int16)\n#     user_sum2 = np.zeros(question_len, dtype=np.int16)\n#     user_count2 = np.zeros(question_len, dtype=np.int16)\n\n#     user_sum_dict_test=defaultdict(int)\n#     user_count_dict_test=defaultdict(int)\n\n    task_container_sum = np.zeros(question_len, dtype=np.int32)\n    task_container_count = np.zeros(question_len, dtype=np.int32)\n    task_container_std = np.zeros(question_len, dtype=np.float16)\n\n    explanation_sum = np.zeros(question_len, dtype=np.int32)\n    explanation_count = np.zeros(question_len, dtype=np.int32)\n#     delta_prior_question_elapsed_time = np.zeros(question_len, dtype=np.int32)\n\n    attempt_no_count = np.zeros(question_len, dtype=np.int16)\n    lagtime = np.zeros(question_len, dtype=np.float32)\n    lagtime2 = np.zeros(question_len, dtype=np.float32)\n    lagtime3 = np.zeros(question_len, dtype=np.float32)\n    lagtime_in_lecture = np.zeros(question_len, dtype=np.float32)\n    part_lagtime = np.zeros(question_len, dtype=np.float32)\n#     content_lagtime = np.zeros(question_len, dtype=np.float32)\n    #lagtime_means = np.zeros(question_len, dtype=np.int32)\n    #\n    \n   \n    i=0\n    for j, (user_id,prior_question_had_explanation,content_type_id,prior_question_elapsed_time,timestamp, content_id,task_container_id, attempt_no, part) in enumerate(zip(test_df['user_id'].values,test_df['prior_question_had_explanation'].values,test_df['content_type_id'].values,test_df['prior_question_elapsed_time'].values,test_df['timestamp'].values, test_df['content_id'].values, test_df['task_container_id'].values, test_df[\"attempt_no\"].values, test_df[\"part\"].values)):\n        \n         #\n#         user_lecture_sum_dict[user_id] += content_type_id\n#         user_lecture_count_dict[user_id] += 1\n        if(content_type_id==1):#\n            x=1\n            max_timestamp_u_dict_c['max_time_stamp'][user_id]=timestamp\n            k = (user_id, part)\n            if k in part_lecture_user_sum_dict.keys():\n                part_lecture_user_sum_dict[k] += 1\n                part_lecture_user_count_dict[k] += 1\n            else:\n                part_lecture_user_sum_dict[k] = 1\n                part_lecture_user_count_dict[k] = 1\n#             if(len(user_lecture_stats_part[user_lecture_stats_part.user_id==user_id])==0):\n#                 user_lecture_stats_part = user_lecture_stats_part.append([{'user_id':user_id}], ignore_index=True)\n#                 user_lecture_stats_part.fillna(0, inplace=True)\n#                 user_lecture_stats_part.loc[user_lecture_stats_part.user_id==user_id,part_lectures_columns + types_of_lectures_columns]+=lectures_df[lectures_df.lecture_id==content_id][part_lectures_columns + types_of_lectures_columns].values\n#             else:\n#                 user_lecture_stats_part.loc[user_lecture_stats_part.user_id==user_id,part_lectures_columns + types_of_lectures_columns]+=lectures_df[lectures_df.lecture_id==content_id][part_lectures_columns + types_of_lectures_columns].values\n        if(content_type_id==0):#   \n#             user_lecture_sum[i] = user_lecture_sum_dict[user_id]\n#             user_lecture_count[i] = user_lecture_count_dict[user_id]\n#             lagtime_sum[i] = lagtime_sum_dict[user_id]\n#             lagtime_count[i] = lagtime_count_dict[user_id]\n    \n            user_sum[i] = user_sum_dict[user_id]\n            user_count[i] = user_count_dict[user_id]\n            attempt_sum[i] = attempt_cum_sum_dict[user_id]\n            attempt_count[i] = attempt_cum_count_dict[user_id]\n#             user_sum2[i] = user_sum_dict2[user_id]\n#             user_count2[i] = user_count_dict2[user_id]\n    #         content_sum[i] = content_sum_dict[content_id]\n    #         content_count[i] = content_count_dict[content_id]\n            task_container_sum[i] = task_container_sum_dict[task_container_id]\n            task_container_count[i] = task_container_count_dict[task_container_id]\n            task_container_std[i]=task_container_std_dict[task_container_id]\n\n            explanation_sum_dict[user_id] += prior_question_had_explanation\n            explanation_count_dict[user_id] += 1\n            explanation_sum[i] = explanation_sum_dict[user_id]\n            explanation_count[i] = explanation_count_dict[user_id]\n            \n            k = (user_id, part)\n            if k in max_timestamp_u_part_dict.keys():\n                tmp = max_timestamp_u_part_dict[k]\n                part_lagtime[i] = timestamp-tmp\n                max_timestamp_u_part_dict[k] = timestamp\n            else:\n                part_lagtime[i] = -1\n                max_timestamp_u_part_dict[k] = timestamp\n            part_user_sum[i] = part_user_sum_dict[k]\n            part_user_count[i] = part_user_count_dict[k]\n            \n            if k in part_lecture_user_sum_dict.keys():\n                part_lecture_user_count_dict[k] += 1\n            else:\n                part_lecture_user_count_dict[k] = 1\n            part_lecture_user_sum[i] = part_lecture_user_sum_dict[k]\n            part_lecture_user_count[i] = part_lecture_user_count_dict[k]\n\n            if user_id in max_timestamp_u_dict['max_time_stamp'].keys():\n                lagtime[i]=timestamp-max_timestamp_u_dict['max_time_stamp'][user_id]\n                lagtime_in_lecture[i]=timestamp-max_timestamp_u_dict_c['max_time_stamp'][user_id]\n                if(max_timestamp_u_dict2['max_time_stamp2'][user_id]==lagtime_mean2):#\n                    lagtime2[i]=lagtime_mean2\n                    lagtime3[i]=lagtime_mean3\n                    #max_timestamp_u_dict3['max_time_stamp3'].update({user_id:lagtime_mean3})\n                else:\n                    lagtime2[i]=timestamp-max_timestamp_u_dict2['max_time_stamp2'][user_id]\n                    if(max_timestamp_u_dict3['max_time_stamp3'][user_id]==lagtime_mean3):\n                        lagtime3[i]=lagtime_mean3 #\n                    else:\n                        lagtime3[i]=timestamp-max_timestamp_u_dict3['max_time_stamp3'][user_id]\n                    \n                    max_timestamp_u_dict3['max_time_stamp3'][user_id]=max_timestamp_u_dict2['max_time_stamp2'][user_id]\n                        \n                max_timestamp_u_dict2['max_time_stamp2'][user_id]=max_timestamp_u_dict['max_time_stamp'][user_id]\n                max_timestamp_u_dict['max_time_stamp'][user_id]=timestamp\n#                 lagtime_means[i]=(lagtime_mean_dict[user_id]+lagtime[i])/2\n#                 lagtime_mean_dict[user_id]=lagtime_means[i]\n            else:\n                lagtime[i]=lagtime_mean\n                lagtime_in_lecture[i] = lagtime_in_lecture_mean\n                max_timestamp_u_dict['max_time_stamp'].update({user_id:timestamp})\n                lagtime2[i]=lagtime_mean2#\n                max_timestamp_u_dict2['max_time_stamp2'].update({user_id:lagtime_mean2})\n                lagtime3[i]=lagtime_mean3#\n                max_timestamp_u_dict3['max_time_stamp3'].update({user_id:lagtime_mean3})\n#                 lagtime_mean_dict.update({user_id:lagtime_mean})\n#                 lagtime_means[i]=(lagtime_mean_dict[user_id]+lagtime[i])/2\n            \n#             tmp = max_timestamp_u_content[(max_timestamp_u_content.user_id==user_id) & (max_timestamp_u_content.content_id==content_id)]\n#             if len(tmp)==1:\n#                 content_lagtime[i] = timestamp - tmp.timestamp.values[0]\n#                 max_timestamp_u_content.loc[(max_timestamp_u_content.user_id==user_id) & (max_timestamp_u_content.content_id==content_id), 'timestamp'] = timestamp\n#             else:\n#                 content_lagtime[i] = -1\n#                 max_timestamp_u_content.append({'user_id': user_id, 'content_id': content_id, 'timestamp': timestamp}, ignore_index=True)\n\n            if user_id in user_prior_question_elapsed_time_dict['prior_question_elapsed_time'].keys():            \n#                 delta_prior_question_elapsed_time[i]=prior_question_elapsed_time-user_prior_question_elapsed_time_dict['prior_question_elapsed_time'][user_id]\n                user_prior_question_elapsed_time_dict['prior_question_elapsed_time'][user_id]=prior_question_elapsed_time\n            else:           \n#                 delta_prior_question_elapsed_time[i]=delta_prior_question_elapsed_time_mean    \n                user_prior_question_elapsed_time_dict['prior_question_elapsed_time'].update({user_id:prior_question_elapsed_time})\n            i=i+1 \n\n\n        \n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n    \n    #HDKIM SAKT\n    test_dataset = TestDataset(group, test_df, n_skill, max_seq=MAX_SEQ)\n    test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n    SAKT_outs = []\n    \n    for item in test_dataloader:\n        x = item[0].to(device).long()\n        target_id = item[1].to(device).long()\n\n        with torch.no_grad():\n            output, att_weight = sakt_model(x, target_id)\n \n        output = torch.sigmoid(output)\n        output = output[:, -1]\n        SAKT_outs.extend(output.view(-1).data.cpu().numpy())\n    \n    #test_df = test_df[test_df['content_type_id'] == 0]\n    #right_index=True\n    #test_df = pd.merge(test_df, questions_df, on='content_id', how='left',right_index=True)    \n    #test_df = pd.concat([test_df.reset_index(drop=True), questions_df.reindex(test_df['content_id'].values).reset_index(drop=True)], axis=1)\n    test_df=test_df.merge(questions_df.loc[questions_df.index.isin(test_df['content_id'])],\n                  how='left', on='content_id', right_index=True)\n    \n    #test_df = pd.merge(test_df, user_lecture_stats_part, on=['user_id'], how=\"left\",right_index=True)\n    #test_df = pd.concat([test_df.reset_index(drop=True), user_lecture_stats_part.reindex(test_df['user_id'].values).reset_index(drop=True)], axis=1)\n#     test_df=test_df.merge(user_lecture_stats_part.loc[user_lecture_stats_part.index.isin(test_df['user_id'])],\n#                   how='left', on='user_id', right_index=True)\n \n#     test_df['user_lecture_lv'] = user_lecture_sum / user_lecture_count\n#     test_df['user_lecture_sum'] = user_lecture_sum\n    \n#     test_df['user_interaction_count'] = user_lecture_count\n#     test_df['user_interaction_timestamp_mean'] = test_df['timestamp']/user_lecture_count\n\n#     test_df['lagtime_cumsum'] = lagtime_sum\n#     test_df['user_interaction_lagtime_mean'] = lagtime_sum / lagtime_count\n    \n    test_df['user_correctness'] = user_sum / user_count\n    test_df['user_uncorrect_count'] =user_count-user_sum\n    test_df['user_correct_count'] =user_sum\n    test_df['attempt_no_mean'] = attempt_sum / attempt_count\n    test_df['user_part_correctness'] = part_user_sum / part_user_count\n    test_df['user_part_rate'] = part_user_count / user_count\n    test_df['user_part_correctness'] = part_user_sum / part_user_count\n    test_df['user_part_rate'] = part_user_count / user_count\n    test_df['user_lecture_sum'] = part_lecture_user_sum\n    test_df['user_lecture_lv'] = part_lecture_user_sum / part_lecture_user_count\n    \n    #test_df['user_answer_count'] =user_count\n    \n#     test_df['user_correctness2'] = user_sum2 / user_count2\n#     test_df['user_uncorrect_count2'] =user_count2-user_sum2\n#     test_df['user_correct_count2'] =user_sum2\n    #test_df['user_answer_count2'] =user_count2\n    \n    #    \n#     test_df['task_container_correctness'] = task_container_sum / task_container_count\n#     test_df['task_container_cor_count'] = task_container_sum \n    test_df['task_container_uncor_count'] =task_container_count-task_container_sum \n#     test_df['task_container_std'] = task_container_std \n    #test_df['content_task_mean'] = content_task_mean \n    \n#     test_df['explanation_mean'] = explanation_sum / explanation_count\n#     test_df['explanation_true_count'] = explanation_sum\n    test_df['explanation_false_count'] = explanation_count-explanation_sum \n    \n    #\n#     test_df['delta_prior_question_elapsed_time'] = delta_prior_question_elapsed_time \n    \n  \n    # test_df[\"attempt_no\"] = test_df[[\"user_id\", \"content_id\"]].apply(lambda row: get_max_attempt(row[\"user_id\"], row[\"content_id\"]), axis=1)\n    test_df[\"lagtime\"]=lagtime\n    test_df[\"lagtime2\"]=lagtime2\n    test_df[\"lagtime3\"]=lagtime3\n    test_df[\"lagtime_in_lecture\"]=lagtime_in_lecture\n    test_df['part_lagtime'] = part_lagtime\n    test_df[\"content_lagtime\"] = test_df[[\"user_id\", \"content_id\", \"timestamp\"]].apply(lambda row: get_content_lagtime(row[\"user_id\"], row[\"content_id\"], row[\"timestamp\"]), axis=1)\n    \n    #test_df[\"lagtime_mean\"]=lagtime_means\n    test_df['lag_elapsed_time'] = test_df['lagtime'] - test_df['prior_question_elapsed_time']\n    test_df['lag_elapsed_time'].fillna(lag_elapsed_time_mean, inplace=True)\n\n    \n\n    test_df['timestamp']=test_df['timestamp']/(1000*3600)\n    test_df.timestamp=test_df.timestamp.astype('float16')\n    test_df['lagtime']=test_df['lagtime']/(1000*3600)\n    test_df.lagtime=test_df.lagtime.astype('float32')\n    test_df['lagtime2']=test_df['lagtime2']/(1000*3600)\n    test_df.lagtime2=test_df.lagtime2.astype('float32')\n    test_df['lagtime3']=test_df['lagtime3']/(1000*3600)\n    test_df.lagtime3=test_df.lagtime3.astype('float32')\n    test_df['lagtime_in_lecture']=test_df['lagtime_in_lecture']/(1000*3600)\n    test_df.lagtime_in_lecture=test_df.lagtime_in_lecture.astype('float32')\n    test_df['content_lagtime']=test_df['content_lagtime']/(1000*3600)\n    test_df.content_lagtime=test_df.content_lagtime.astype('float32')\n    test_df['part_lagtime']=test_df['part_lagtime']/(1000*3600)\n    test_df.part_lagtime=test_df.part_lagtime.astype('float32')\n    \n    \n    test_df['lagtime1_2'] = test_df['lagtime'] - test_df['lagtime2']\n    test_df['lagtime2_3'] = test_df['lagtime2'] - test_df['lagtime3']\n    \n    \n#     test_df['user_interaction_timestamp_mean']=test_df['user_interaction_timestamp_mean']/(1000*3600)\n#     test_df.user_interaction_timestamp_mean=test_df.user_interaction_timestamp_mean.astype('float32')\n    \n#     test_df['user_interaction_lagtime_mean'] = test_df[\"lagtime\"]/user_count\n#     test_df['user_interaction_lagtime_mean'] = test_df['user_interaction_lagtime_mean']/(1000*3600)\n#     test_df.user_interaction_lagtime_mean = test_df.user_interaction_lagtime_mean.astype('float32')\n    \n    test_df['user_correctness'].fillna(0.67, inplace=True)\n    #test_df['user_correctness2'].fillna(0.67, inplace=True)\n    #\n    #test_df = test_df.astype(features_dict)\n\n    sub_preds = np.zeros(test_df.shape[0])\n    for i, model in enumerate(clfs, 1):\n        test_preds  = model.predict(test_df[features])\n        sub_preds += test_preds\n    lgbm_final = sub_preds / len(clfs)\n    test_df[target] = np.array(SAKT_outs) * 0.5 + lgbm_final * 0.5\n#     test_df[target] = lgbm_final\n    \n#     if(flag_lgbm):\n#         test_df[target] = model.predict(test_df[features])\n#     else:\n#         test_df[target] = model.predict(test_df[features].values)\n    env.predict(test_df[['row_id', target]])","execution_count":34,"outputs":[{"output_type":"stream","text":"pre:  18\naft:  18\npre:  27\naft:  27\npre:  26\naft:  26\npre:  33\naft:  33\nCPU times: user 1.82 s, sys: 728 ms, total: 2.54 s\nWall time: 1.4 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}